import bs4 as bs
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException
import requests
import random
from bs4 import BeautifulSoup
import pandas as pd
import regex as re
import numpy as np
import ast

with open('war.txt', 'r') as f:
    war = ast.literal_eval(str(f.readlines()).lower().replace('\\n', ''))
war = np.unique(war)
war = list(war[war != ''])
ua_words_list = ['донбас', 'донецк', 'дончан', 'киев', 'крым', 'луганск', 'луганч', 'украин', 'лнр', 'днр']
war_words_list = ['бандер', 'бендер', 'блицкриг', 'геноцид', 'денацифи', 'коллаборант', 'коллаборацион', 'наци',  'нео-наци', 'неонаци', 'фаши']
all_matches = []
for ind, i in enumerate(war_words_list):
    matching = [s for s in war if i in s]
    all_matches.extend(matching)

war_list = list(set(war) ^ set(all_matches))
war_words_list = war_words_list + war_list

link='http://www.kremlin.ru/'

chrome_path = "./chromedriver"

list_of_links = pd.read_csv('list_of_links.csv')
print(list_of_links)
list_of_links = list(list_of_links['link'])
print(list_of_links)

screen_sizes = [[1366,768], [1920,1080]]

countries = ['KZ', 'RU', 'UZ', 'BY']

def get_proxy():
    response = requests.get('https://sslproxies.org/')
    soup = BeautifulSoup(response.text, 'lxml')
    tag = 'textarea'
    proxies = soup.find_all(tag)

    proxies_tb = soup.find_all('tr')[1:]
    proxies = str(proxies).split('\n')
    proxies_tb = [i.find_all('td') for i in proxies_tb]
    proxies_tb = [i for i in proxies_tb if len(i) > 0]
    proxies_tb = proxies_tb[:len(proxies)]
    codes = [i[2].text for i in proxies_tb]
    ids = [i[0].text for i in proxies_tb]
    proxies_dict = dict(zip(ids, codes))

    listOfKeys = [key for (key, value) in proxies_dict.items() if value in countries]
    proxies = [i for i in proxies if ':' in i][1:]

    new_p = []
    for j in listOfKeys:
        proxies1 = [i for i in proxies if str(j) in str(i)]
        new_p.extend(proxies1)

    return new_p


def set_driver(proxies=False):
    chrome_options = webdriver.ChromeOptions()
    if proxies != False:
        if len(proxies) > 0:
            chrome_options.add_argument('--proxy-server={}'.format(proxies[0]))
            proxies.pop(0)
        else:
            proxies = get_proxy()
            chrome_options.add_argument('--proxy-server={}'.format(proxies[0]))
    else:
        proxies = get_proxy()
        chrome_options.add_argument('--proxy-server={}'.format(proxies[0]))
        proxies.pop(0)
    print('proxies added', proxies)
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-extensions')
    chrome_options.add_argument("--disable-infobars")
    chrome_options.add_argument('--profile-directory=Default')
    chrome_options.add_argument("--incognito")
    chrome_options.add_argument("--disable-plugins-discovery")
    chrome_options.add_argument("--start-maximized")
    # chrome_options.headless = True
    chrome_options.add_argument("--disable-gpu")


    chrome_options.add_argument("enable-automation")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-browser-side-navigation")
    # chrome_options.setPageLoadStrategy(PageLoadStrategy.NORMAL)


    driver = webdriver.Chrome(chrome_path, options=chrome_options)
    driver.delete_all_cookies()
    # driver.set_window_size(1000, 800)
    # driver.set_window_position(0, 0)
    screen = random.choice(screen_sizes)
    driver.set_window_size(screen[0], screen[1])
    chrome_options.add_argument("–disable-dev-shm-usage")
    chrome_options.add_argument("start-maximized")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-setuid-sandbox")
    # chrome_options.add_argument("referer=" + category_page)
    chrome_options.add_argument('accept-encoding=' + 'gzip, deflate, br')
    chrome_options.add_argument(
        'accept=' + 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9')
    chrome_options.add_argument("upgrade-insecure-requests=" + "1")
    chrome_options.add_argument('cookies=' + 'yes')
    chrome_options.add_argument("timezone=" + "-360")
    chrome_options.add_argument("languages-js=" + "pt-BR,en-US,pt,en")
    chrome_options.add_argument(
        "plugins=" + "Plugin 0: Chrome PDF Plugin; Portable Document Format; internal-pdf-viewer. Plugin 1: Chrome PDF Viewer; ; mhjfbmdgcfjbbpaeojofohoefgiehjai. Plugin 2: Native Client; ; internal-nacl-plugin. ")
    chrome_options.add_argument("screen_depth=" + "24")
    chrome_options.add_argument("screen_left" + "0")
    chrome_options.add_argument("screen_top" + "0")
    chrome_options.add_argument('accept-language' + 'pt-BR,pt;q=0.9,en-US,en;q=0.8')
    connected = False
    while not connected:
        print('try connections')
        try:
            print('try connection')
            driver = webdriver.Chrome(chrome_path, options=chrome_options)
            connected = True
        except (NoSuchElementException, TimeoutException, WebDriverException) as e:
            print('no connection x')
            pass
    return driver, proxies


def try_driver(proxies, link):
    connected = False
    try:
        driver, proxies = set_driver(proxies)
        while not connected:
            try:
                driver.get(link)
                connected = True
            except:
                driver, proxies = set_driver(proxies)
    except:
        driver, proxies = set_driver(proxies)
        while not connected:
            try:
                driver.get(link)
                connected = True
            except:
                driver, proxies = set_driver(proxies)

    return driver, proxies


def try_proxy(driver, proxies, link, list_of_links):
    soup = ''
    page = ''
    get_articles(driver, link, proxies, list_of_links)
    return driver, soup, proxies, page

def find_words(text):
    text = str(text).lower().replace('\\n', ' ').replace('\\', ' ').replace('xa0', ' ')
    ua_words = []
    for word in ua_words_list:
        ua = re.findall(r'\b'+word+'\w+', text)
        ua_words.extend(ua)
    ua_words = np.unique(ua_words)

    war_words = []
    for word in war_words_list:
        war = re.findall(r'\b'+word+'\w+', text)
        war_words.extend(war)
    war_words = np.unique(war_words)

    return ua_words, war_words


def get_articles(driver, link, proxies, list_of_links):
    driver.get(link)

    articles_link = 'http://www.kremlin.ru/events/president/news/'

    driver, proxies = set_driver(proxies)
    df = pd.DataFrame()
    for i in list_of_links:

        try:
            # TODO: fix error
            try:
                driver.get(articles_link + str(i))
            except:
                driver.get(articles_link + str(i))
            driver.execute_script("window.stop();")
            page = driver.page_source
            soup = bs.BeautifulSoup(page, 'html.parser')
            try:
                text = soup.find("article").text
                if 'Forbidden' in text:
                    try:
                        driver, proxies = set_driver(proxies)
                        print(i)
                        driver.get(articles_link + str(i))
                        driver.execute_script("window.stop();")

                        page = driver.page_source
                        soup = bs.BeautifulSoup(page, 'html.parser')

                        try:
                            text = soup.find("article").text
                        except:
                            text = ''
                    except:
                        text = ''

            except:
                try:
                    driver, proxies = set_driver(proxies)
                    try:
                        driver.get(articles_link + str(i))
                    except:
                        driver.get(articles_link + str(i))
                    driver.execute_script("window.stop();")
                    page = driver.page_source
                    soup = bs.BeautifulSoup(page, 'html.parser')

                    try:
                        text = soup.find("article").text
                    except:
                        text = ''
                except:
                    text = ''

            try:
                date_of_article = soup.find("time").attrs['datetime']
            except:
                date_of_article = ''

            ua_words, war_words = find_words(text)
            print(ua_words, war_words)

            if len(war_words)>0:
                saved_text = text
            else:
                saved_text = ''

            infodict = {'id': i, 'date':date_of_article, 'text':saved_text, 'ua':str(ua_words), 'war':str(war_words)}

        except:
            driver, proxies = set_driver(proxies)
            driver.get(articles_link + str(i))
            driver.execute_script("window.stop();")

            page = driver.page_source
            soup = bs.BeautifulSoup(page, 'html.parser')

            try:
                text = soup.find("article").text
            except:
                text = ''

            try:
                date_of_article = soup.find("time").attrs['datetime']
            except:
                date_of_article = ''

            ua_words, war_words = find_words(text)

            if len(war_words) > 0:
                saved_text = text
            else:
                saved_text = ''

            infodict = {'id': i, 'date': date_of_article, 'text': saved_text, 'ua': str(ua_words), 'war': str(war_words)}

        if infodict['date']=='':
            try:
                driver.get(articles_link + str(i))
                driver.execute_script("window.stop();")

                page = driver.page_source
                soup = bs.BeautifulSoup(page, 'html.parser')
                try:
                    text = soup.find("article").text
                except:
                    text = ''

                try:
                    date_of_article = soup.find("time").attrs['datetime']
                except:
                    date_of_article = ''

                ua_words, war_words = find_words(text)

                if len(war_words) > 0:
                    saved_text = text
                else:
                    saved_text = ''

                infodict = {'id': i, 'date': date_of_article, 'text': saved_text, 'ua': str(ua_words),
                            'war': str(war_words)}
            except:
                driver, proxies = set_driver(proxies)
                tt = 0
                while tt==0:
                    try:
                        driver.get(articles_link + str(i))
                        tt=1
                    except:
                        pass
                driver.execute_script("window.stop();")
                page = driver.page_source
                soup = bs.BeautifulSoup(page, 'html.parser')

                try:
                    text = soup.find("article").text
                except:
                    text = ''

                try:
                    date_of_article = soup.find("time").attrs['datetime']
                except:
                    date_of_article = ''

                ua_words, war_words = find_words(text)

                if len(war_words) > 0:
                    saved_text = text
                else:
                    saved_text = ''

                infodict = {'id': i, 'date': date_of_article, 'text': saved_text, 'ua': str(ua_words),
                            'war': str(war_words)}


        df = df.append(pd.DataFrame([infodict]))
        df.to_csv('articles.csv', index=False)






    return df

proxies = get_proxy()
driver, proxies = set_driver(proxies)

try_proxy(driver, proxies, link, list_of_links)
